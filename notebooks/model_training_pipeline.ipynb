{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Anonymization Model Training Pipeline\n",
    "\n",
    "This notebook demonstrates how to fine-tune a T5 model for text anonymization using the Hugging Face ecosystem. The pipeline includes:\n",
    "\n",
    "1. Loading and preprocessing synthetic data\n",
    "2. Setting up the model and tokenizer\n",
    "3. Training configuration and process\n",
    "4. Model evaluation and metrics\n",
    "5. Inference examples\n",
    "\n",
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import torch\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# # Set random seed for reproducibility\n",
    "# torch.manual_seed(42)\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "We'll use our synthetic dataset from the Hugging Face Hub. This dataset contains pairs of original and anonymized texts, perfect for training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"kurkowski/synthetic-contextual-anonymizer-dataset\")\n",
    "\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"Training examples: {len(dataset['train'])}\")\n",
    "print(f\"Validation examples: {len(dataset['validation'])}\")\n",
    "print(f\"Test examples: {len(dataset['test'])}\")\n",
    "\n",
    "\n",
    "# Display a sample\n",
    "print(\"\\nSample from training set:\")\n",
    "print(\"Original:\", dataset['train'][0]['context'])\n",
    "print(\"Anonymized:\", dataset['train'][0]['anonymized_context'])\n",
    "print(\"Used labels:\", dataset['train'][0]['used_labels'], type(dataset['train'][0]['used_labels']))\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Setup\n",
    "\n",
    "We'll use FLAN-T5-small as our base model. This is a good choice because:\n",
    "- It's relatively small and fast to train\n",
    "- It has good text-to-text capabilities\n",
    "- It's been trained on a variety of tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and model\n",
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "We need to convert our text examples into a format suitable for the model. This includes:\n",
    "- Adding a task-specific prompt\n",
    "- Tokenizing inputs and targets\n",
    "- Creating attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_labels(text):\n",
    "    \"\"\"\n",
    "    Usuwa numerację z etykiet (np. [NAME_1] -> [NAME]).\n",
    "    Obsługuje również etykiety z podkreślnikami w nazwie (np. POLICY_NUMBER).\n",
    "    \"\"\"\n",
    "    if isinstance(text, list):\n",
    "        return [normalize_labels(t) for t in text]\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    return re.sub(r'\\[([A-Z_]+)_\\d+\\]', r'[\\1]', text)\n",
    "\n",
    "def create_anonymization_prompt(labels):\n",
    "    \"\"\"Create a prompt for text anonymization task.\n",
    "    \n",
    "    Args:\n",
    "        labels: List of labels to use for anonymization\n",
    "        \n",
    "    Returns:\n",
    "        String containing the formatted prompt\n",
    "    \"\"\"\n",
    "    return f\"\"\"You are a text anonymization expert. Your task is to replace sensitive information with the following labels: { normalize_labels(labels)}.\n",
    "\n",
    "    Instructions:\n",
    "    1. Replace each sensitive information with appropriate label from the provided list\n",
    "    2. For multiple occurrences of the same type, use numbered labels (e.g. [NAME_1], [NAME_2])\n",
    "    3. Preserve the original text structure and meaning\n",
    "    4. Follow the examples precisely\n",
    "\n",
    "    Example:\n",
    "    Input: \"John Smith called Mary Johnson. John's number is 555-0123 and Mary's is 555-4567.\"\n",
    "    Output: \"[NAME_1] called [NAME_2]. [NAME_1]'s number is [PHONE_1] and [NAME_2]'s is [PHONE_2].\"\n",
    "\n",
    "    Task:\n",
    "    Anonymize the following text using only these labels: { normalize_labels(labels)}\n",
    "    Input: \n",
    "    \"\"\"\n",
    "\n",
    "def convert_examples_to_features(example_batch):\n",
    "    \"\"\"Convert text examples to model features.\n",
    "    \n",
    "    Args:\n",
    "        example_batch: Batch of examples from dataset\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with input_ids, attention_mask, and labels\n",
    "    \"\"\"\n",
    "    input_texts = []\n",
    "    for text, labels in zip(example_batch[\"context\"], example_batch[\"used_labels\"]):\n",
    "        prompt = create_anonymization_prompt(labels)\n",
    "        input_texts.append(prompt + text)\n",
    "    \n",
    "    print('input_texts:')\n",
    "    for text in input_texts[:3]:\n",
    "        print(text)\n",
    "    input_encodings = tokenizer(input_texts, truncation=True, padding=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(example_batch[\"anonymized_context\"], truncation=True, padding=True)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_encodings[\"input_ids\"],\n",
    "        \"attention_mask\": input_encodings[\"attention_mask\"], \n",
    "        \"labels\": target_encodings[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "test_prompt = create_anonymization_prompt(dataset['train'][0]['used_labels'])\n",
    "\n",
    "# Process all splits\n",
    "processed_dataset = dataset.map(\n",
    "    convert_examples_to_features,\n",
    "    batched=True,\n",
    "    desc=\"Processing dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_anonymization(text_to_anonymize, labels):\n",
    "    prompt = create_anonymization_prompt(labels)\n",
    "    print(prompt + text_to_anonymize)\n",
    "    inputs = tokenizer(\n",
    "        prompt + text_to_anonymize, \n",
    "        return_tensors=\"pt\",  # Tutaj jest OK używać return_tensors=\"pt\"\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=512,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print(test_anonymization(processed_dataset['train'][0]['context'], processed_dataset['train'][0]['used_labels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Configuration\n",
    "\n",
    "We'll configure the training process with optimal parameters for our task. Key considerations include:\n",
    "- Memory efficiency (batch size and gradient accumulation)\n",
    "- Learning rate and warmup\n",
    "- Evaluation strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "trainer_args = TrainingArguments(\n",
    "    output_dir = \"anonymizer_model\", \n",
    "    num_train_epochs=3, \n",
    "    warmup_steps = 500,\n",
    "    per_device_train_batch_size=4,      \n",
    "    per_device_eval_batch_size=4,      \n",
    "    weight_decay=0.01, \n",
    "    logging_steps=2,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=250,\n",
    "    save_steps=250,\n",
    "    gradient_accumulation_steps=2,      \n",
    "    learning_rate=5e-5,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_first_step=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    gradient_checkpointing=True,        \n",
    "    torch_compile=False,                \n",
    "    dataloader_pin_memory=False,      \n",
    "    torch_empty_cache_steps=2         \n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "if hasattr(torch.mps, 'empty_cache'):\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=trainer_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=seq2seq_data_collator,\n",
    "    train_dataset=processed_dataset[\"train\"],\n",
    "    eval_dataset=processed_dataset[\"validation\"]\n",
    ")\n",
    "\n",
    "\n",
    "train_result =trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n",
    "\n",
    "We'll evaluate our anonymization model using multiple metrics that provide a comprehensive assessment:\n",
    "\n",
    "1. **Entity-level metrics** - Precision, Recall, F1 for correctly anonymized data\n",
    "2. **Text similarity metrics** - BLEU and ROUGE-L to measure text structure preservation\n",
    "3. **Data privacy metrics** - Ensuring sensitive information doesn't appear in predictions\n",
    "4. **Visualization** - Charts to understand model performance\n",
    "\n",
    "Let's start by loading the model from our latest checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model and tokenizer\n",
    "model_name = \"google/flan-t5-small\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"anonymizer_model/checkpoint-1250\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Set, Tuple\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from nltk import download\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Download NLTK resources if needed\n",
    "download('punkt')\n",
    "\n",
    "\n",
    "def extract_sensitive_data(original_text: str, anonymized_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts sensitive data pairs from original and anonymized text.\n",
    "    \n",
    "    Args:\n",
    "        original_text: The text containing sensitive information\n",
    "        anonymized_text: The anonymized version with labels\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with sensitive data pairs, values, labels and types\n",
    "    \"\"\"\n",
    "    # Label pattern\n",
    "    label_pattern = r'\\[([A-Z_]+)_\\d+\\]'\n",
    "    \n",
    "    # Split texts into lines for easier comparison\n",
    "    original_lines = original_text.split('\\n')\n",
    "    anonymized_lines = anonymized_text.split('\\n')\n",
    "    \n",
    "    sensitive_pairs = []\n",
    "    \n",
    "    # For each pair of lines\n",
    "    for orig_line, anon_line in zip(original_lines, anonymized_lines):\n",
    "        # Skip identical lines\n",
    "        if orig_line == anon_line:\n",
    "            continue\n",
    "            \n",
    "        # Find all labels in the anonymized line\n",
    "        labels = list(re.finditer(label_pattern, anon_line))\n",
    "        \n",
    "        if not labels:\n",
    "            continue\n",
    "            \n",
    "        # Create a list of text fragments between labels\n",
    "        anon_fragments = []\n",
    "        last_end = 0\n",
    "        \n",
    "        for label_match in labels:\n",
    "            start = label_match.start()\n",
    "            if start > last_end:\n",
    "                anon_fragments.append(anon_line[last_end:start])\n",
    "            anon_fragments.append(label_match.group(0))\n",
    "            last_end = label_match.end()\n",
    "            \n",
    "        if last_end < len(anon_line):\n",
    "            anon_fragments.append(anon_line[last_end:])\n",
    "            \n",
    "        # Remove empty fragments and white space\n",
    "        anon_fragments = [f.strip() for f in anon_fragments if f.strip()]\n",
    "        \n",
    "        # For each label, find the corresponding text\n",
    "        for i, fragment in enumerate(anon_fragments):\n",
    "            if re.match(label_pattern, fragment):\n",
    "                label_type = re.match(r'\\[([A-Z_]+)', fragment).group(1)\n",
    "                \n",
    "                # Look for text before and after the label\n",
    "                before = anon_fragments[i-1] if i > 0 else \"\"\n",
    "                after = anon_fragments[i+1] if i < len(anon_fragments)-1 else \"\"\n",
    "                \n",
    "                # Find the corresponding text in the original line\n",
    "                if before and after:\n",
    "                    pattern = f\"{re.escape(before)}(.*?){re.escape(after)}\"\n",
    "                    if match := re.search(pattern, orig_line):\n",
    "                        value = match.group(1).strip()\n",
    "                        if value:\n",
    "                            sensitive_pairs.append((value, fragment, label_type))\n",
    "                elif before:\n",
    "                    pattern = f\"{re.escape(before)}(.*?)(?={re.escape(after) if after else '$'})\"\n",
    "                    if match := re.search(pattern, orig_line):\n",
    "                        value = match.group(1).strip()\n",
    "                        if value:\n",
    "                            sensitive_pairs.append((value, fragment, label_type))\n",
    "                elif after:\n",
    "                    pattern = f\"(?<=^|{re.escape(before)})(.*?){re.escape(after)}\"\n",
    "                    if match := re.search(pattern, orig_line):\n",
    "                        value = match.group(1).strip()\n",
    "                        if value:\n",
    "                            sensitive_pairs.append((value, fragment, label_type))\n",
    "                else:\n",
    "                    # If we don't have context, look for text that's not in other pairs\n",
    "                    used_values = set(v for v, _, _ in sensitive_pairs)\n",
    "                    orig_words = set(w.strip() for w in re.split(r'[\\s,.]', orig_line) if w.strip())\n",
    "                    remaining = orig_words - used_values\n",
    "                    if remaining:\n",
    "                        value = max(remaining, key=len)  # take the longest remaining text\n",
    "                        sensitive_pairs.append((value, fragment, label_type))\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    sensitive_pairs = [(v, l, t) for v, l, t in sensitive_pairs \n",
    "                      if not (v in seen or seen.add(v))]\n",
    "    \n",
    "    return {\n",
    "        'sensitive_pairs': sensitive_pairs,\n",
    "        'original_values': set(v for v, _, _ in sensitive_pairs),\n",
    "        'anonymized_labels': set(l for _, l, _ in sensitive_pairs),\n",
    "        'label_types': set(t for _, _, t in sensitive_pairs)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for entity recognition, BLEU and ROUGE metrics\n",
    "\n",
    "def extract_entities(text: str):\n",
    "    \"\"\"\n",
    "    Extracts entity labels in shortened form (e.g., returns \"NAME\" from \"[NAME_1]\").\n",
    "    \"\"\"\n",
    "    pattern = r'\\[([A-Z_]+)_\\d+\\]'\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "def evaluate_entity_sequence(gt_text: str, pred_text: str):\n",
    "    \"\"\"\n",
    "    Evaluates entity sequence by comparing lists of entities extracted from ground truth and prediction.\n",
    "    \"\"\"\n",
    "    gt_entities = extract_entities(gt_text)\n",
    "    pred_entities = extract_entities(pred_text)\n",
    "    \n",
    "    # Positional comparison - only check up to the length of the shorter sequence\n",
    "    min_len = min(len(gt_entities), len(pred_entities))\n",
    "    correct = sum(1 for i in range(min_len) if gt_entities[i] == pred_entities[i])\n",
    "    \n",
    "    # If sequences differ in length, treat excess as errors\n",
    "    missing = len(gt_entities) - correct\n",
    "    extra = len(pred_entities) - correct\n",
    "    \n",
    "    precision = correct / len(pred_entities) if pred_entities else 1.0\n",
    "    recall = correct / len(gt_entities) if gt_entities else 1.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"gt_entities\": gt_entities,\n",
    "        \"pred_entities\": pred_entities,\n",
    "        \"correct\": correct,\n",
    "        \"missing\": missing if missing > 0 else 0,\n",
    "        \"extra\": extra if extra > 0 else 0,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "def compute_bleu(gt_text: str, pred_text: str):\n",
    "    \"\"\"\n",
    "    Calculates BLEU for comparing entire text (using space tokenization).\n",
    "    \"\"\"\n",
    "    gt_tokens = gt_text.split()\n",
    "    pred_tokens = pred_text.split()\n",
    "    smoothie = SmoothingFunction().method4  # Use smoothing to avoid zero results\n",
    "    bleu_score = sentence_bleu([gt_tokens], pred_tokens, smoothing_function=smoothie)\n",
    "    return bleu_score\n",
    "\n",
    "def lcs_length(a, b):\n",
    "    \"\"\"\n",
    "    Calculates length of the longest common subsequence (LCS).\n",
    "    \"\"\"\n",
    "    dp = [[0]*(len(b)+1) for _ in range(len(a)+1)]\n",
    "    for i in range(len(a)):\n",
    "        for j in range(len(b)):\n",
    "            if a[i] == b[j]:\n",
    "                dp[i+1][j+1] = dp[i][j] + 1\n",
    "            else:\n",
    "                dp[i+1][j+1] = max(dp[i][j+1], dp[i+1][j])\n",
    "    return dp[len(a)][len(b)]\n",
    "\n",
    "def compute_rouge_l(gt_text: str, pred_text: str):\n",
    "    \"\"\"\n",
    "    Simple calculation of ROUGE-L metric (based on LCS).\n",
    "    \"\"\"\n",
    "    gt_tokens = gt_text.split()\n",
    "    pred_tokens = pred_text.split()\n",
    "    lcs = lcs_length(gt_tokens, pred_tokens)\n",
    "    rouge_l_recall = lcs / len(gt_tokens) if gt_tokens else 0\n",
    "    rouge_l_precision = lcs / len(pred_tokens) if pred_tokens else 0\n",
    "    if rouge_l_precision + rouge_l_recall == 0:\n",
    "        rouge_l_f1 = 0\n",
    "    else:\n",
    "        rouge_l_f1 = 2 * rouge_l_precision * rouge_l_recall / (rouge_l_precision + rouge_l_recall)\n",
    "    return {\n",
    "        \"rouge_l_precision\": rouge_l_precision,\n",
    "        \"rouge_l_recall\": rouge_l_recall,\n",
    "        \"rouge_l_f1\": rouge_l_f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_privacy_leaks(original_text, ground_truth, prediction):\n",
    "    \"\"\"\n",
    "    Comprehensive privacy leak detection using multiple methods.\n",
    "    \n",
    "    Args:\n",
    "        original_text: The text containing sensitive information\n",
    "        ground_truth: The correctly anonymized version\n",
    "        prediction: The model's output to check for leaks\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (leak_detected: bool, leaked_items: list)\n",
    "    \"\"\"\n",
    "    # Text normalization\n",
    "    def normalize(text):\n",
    "        return re.sub(r'\\s+', ' ', text.lower().strip())\n",
    "    \n",
    "    orig_norm = normalize(original_text)\n",
    "    gt_norm = normalize(ground_truth)\n",
    "    pred_norm = normalize(prediction)\n",
    "    \n",
    "    # Method 1: Using extract_sensitive_data\n",
    "    extracted = extract_sensitive_data(original_text, ground_truth)\n",
    "    sensitive_values = extracted['original_values']\n",
    "\n",
    "    # Check for presence in prediction\n",
    "    leaks = [value for value in sensitive_values \n",
    "             if value and len(value) > 3 and value.lower() in pred_norm]\n",
    "    \n",
    "    return len(leaks) > 0, leaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_anonymization(original_text: str, ground_truth: str, prediction: str):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of anonymization quality using multiple metrics.\n",
    "    \n",
    "    Calculates:\n",
    "      - Entity sequence metrics (precision, recall, F1)\n",
    "      - BLEU score for whole text similarity\n",
    "      - ROUGE-L (F1) for text structure preservation\n",
    "      - Privacy leak detection using multiple methods\n",
    "      \n",
    "    Args:\n",
    "        original_text: The text containing sensitive information\n",
    "        ground_truth: The correctly anonymized version (gold standard)\n",
    "        prediction: The model's anonymized output\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with comprehensive evaluation metrics\n",
    "    \"\"\"\n",
    "    # Entity sequence metrics (correctly placed entity types)\n",
    "    entity_metrics = evaluate_entity_sequence(ground_truth, prediction)\n",
    "    \n",
    "    # Text similarity metrics\n",
    "    bleu = compute_bleu(ground_truth, prediction)\n",
    "    rouge = compute_rouge_l(ground_truth, prediction)\n",
    "    \n",
    "    # Advanced privacy leak detection\n",
    "    has_leaks, leaked_items = detect_privacy_leaks(original_text, ground_truth, prediction)\n",
    "    \n",
    "    return {\n",
    "        \"entity_metrics\": entity_metrics,\n",
    "        \"bleu\": bleu,\n",
    "        \"rouge_l\": rouge,\n",
    "        \"sensitive_data_leak\": has_leaks,\n",
    "        \"leaked_items\": leaked_items\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anonymization(text_to_anonymize, labels):\n",
    "    \"\"\"\n",
    "    Generate anonymized text using the trained model.\n",
    "    \n",
    "    Args:\n",
    "        text_to_anonymize: Original text containing sensitive information\n",
    "        labels: List of entity types to use for anonymization\n",
    "        \n",
    "    Returns:\n",
    "        Anonymized text produced by the model\n",
    "    \"\"\"\n",
    "    prompt = create_anonymization_prompt(labels)\n",
    "    inputs = tokenizer(\n",
    "        prompt + text_to_anonymize, \n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=512,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def evaluate_model_performance(dataset, tokenizer, model, num_examples=5):\n",
    "    \"\"\"\n",
    "    Performs comprehensive evaluation of the anonymization model using multiple metrics.\n",
    "    \n",
    "    This function tests the model on examples from the dataset and calculates:\n",
    "    - Entity recognition metrics (precision, recall, F1)\n",
    "    - Text similarity measures (BLEU, ROUGE-L)\n",
    "    - Privacy leak analysis\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset containing test examples\n",
    "        tokenizer: Model tokenizer\n",
    "        model: Trained anonymization model\n",
    "        num_examples: Number of examples to test\n",
    "        \n",
    "    Returns:\n",
    "        List of evaluation results with detailed metrics for each example\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Get a subset of examples - handle both Dataset objects and lists\n",
    "    if hasattr(dataset[\"test\"], \"select\"):\n",
    "        test_examples = dataset[\"test\"].select(range(min(num_examples, len(dataset[\"test\"]))))\n",
    "    else:\n",
    "        test_examples = dataset[\"test\"][:num_examples]\n",
    "    \n",
    "    print(\"\\n=== Privacy-Preserving Text Anonymization: Model Evaluation ===\\n\")\n",
    "    \n",
    "    for idx, example in enumerate(test_examples):\n",
    "        print(f\"\\n--- Example {idx+1} ---\")\n",
    "        print(\"Original text:\")\n",
    "        print(example['context'])\n",
    "        print(\"\\nGround truth (expected anonymization):\")\n",
    "        print(example['anonymized_context'])\n",
    "        \n",
    "        # Generate model prediction using the prompt created from used_labels\n",
    "        prompt = create_anonymization_prompt(example['used_labels'])\n",
    "        input_text = prompt + example['context']\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True)\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=512,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        print(\"\\nModel prediction:\")\n",
    "        print(prediction)\n",
    "        \n",
    "        metrics = evaluate_anonymization(\n",
    "            example['context'], \n",
    "            example['anonymized_context'], \n",
    "            prediction\n",
    "        )\n",
    "        \n",
    "        print(\"\\nEvaluation metrics:\")\n",
    "        em = metrics[\"entity_metrics\"]\n",
    "        print(f\"Entity Recognition - Precision: {em['precision']:.3f}, Recall: {em['recall']:.3f}, F1: {em['f1']:.3f}\")\n",
    "        print(f\"BLEU Score: {metrics['bleu']:.3f}\")\n",
    "        print(f\"ROUGE-L F1: {metrics['rouge_l']['rouge_l_f1']:.3f}\")\n",
    "        print(f\"Privacy breach detected: {metrics['sensitive_data_leak']}\")\n",
    "        \n",
    "        # Detailed information about leaks\n",
    "        if metrics['sensitive_data_leak']:\n",
    "            print(\"Leaked sensitive data:\")\n",
    "            for item in metrics['leaked_items']:\n",
    "                print(f\"  - '{item}'\")\n",
    "                \n",
    "        # Entity type statistics\n",
    "        if em[\"missing\"] > 0:\n",
    "            print(f\"\\nMissing entity types: {em['missing']}\")\n",
    "        if em[\"extra\"] > 0:\n",
    "            print(f\"Extra entity types: {em['extra']}\")\n",
    "            \n",
    "        results.append(metrics)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_metrics(results):\n",
    "    \"\"\"\n",
    "    Calculates average metrics across all evaluation results.\n",
    "    \n",
    "    Args:\n",
    "        results: List of evaluation results from evaluate_model_performance\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with average metrics and privacy breach percentage\n",
    "    \"\"\"\n",
    "    # Average entity metrics\n",
    "    entity_precisions = [r[\"entity_metrics\"][\"precision\"] for r in results]\n",
    "    entity_recalls = [r[\"entity_metrics\"][\"recall\"] for r in results]\n",
    "    entity_f1s = [r[\"entity_metrics\"][\"f1\"] for r in results]\n",
    "\n",
    "    avg_entity_precision = np.mean(entity_precisions)\n",
    "    avg_entity_recall = np.mean(entity_recalls)\n",
    "    avg_entity_f1 = np.mean(entity_f1s)\n",
    "\n",
    "    # Average BLEU score\n",
    "    bleu_scores = [r[\"bleu\"] for r in results]\n",
    "    avg_bleu = np.mean(bleu_scores)\n",
    "\n",
    "    # Average ROUGE-L F1 score\n",
    "    rouge_l_f1_scores = [r[\"rouge_l\"][\"rouge_l_f1\"] for r in results]\n",
    "    avg_rouge_l_f1 = np.mean(rouge_l_f1_scores)\n",
    "\n",
    "    # Calculate percentage of cases with sensitive data leaks\n",
    "    sensitive_leak_count = sum(1 for r in results if r[\"sensitive_data_leak\"])\n",
    "    leak_percentage = sensitive_leak_count / len(results)\n",
    "    \n",
    "    return {\n",
    "        \"avg_entity_precision\": avg_entity_precision,\n",
    "        \"avg_entity_recall\": avg_entity_recall,\n",
    "        \"avg_entity_f1\": avg_entity_f1,\n",
    "        \"avg_bleu\": avg_bleu,\n",
    "        \"avg_rouge_l_f1\": avg_rouge_l_f1,\n",
    "        \"leak_percentage\": leak_percentage\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_data_privacy_metrics(results, dataset, num_examples=5):\n",
    "    \"\"\"\n",
    "    Calculates global privacy metrics to assess data leakage at scale.\n",
    "    \n",
    "    While per-example leak detection identifies which examples have leaks,\n",
    "    this function provides a global perspective by calculating what percentage\n",
    "    of all sensitive entities were properly anonymized across the entire dataset.\n",
    "    \n",
    "    Args:\n",
    "        results: List of evaluation results from evaluate_model_performance\n",
    "        dataset: Dataset used for evaluation\n",
    "        num_examples: Number of examples that were evaluated\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with comprehensive privacy metrics including:\n",
    "        - total_sensitive_items: Count of all sensitive data elements\n",
    "        - total_leaked_items: Count of sensitive data that leaked\n",
    "        - global_leak_rate: Percentage of all sensitive data that leaked\n",
    "    \"\"\"\n",
    "    total_sensitive_items = 0\n",
    "    total_leaked_items = 0\n",
    "    \n",
    "    # Get the subset of examples that were evaluated\n",
    "    if hasattr(dataset[\"test\"], \"select\"):\n",
    "        test_examples = dataset[\"test\"].select(range(min(num_examples, len(dataset[\"test\"]))))\n",
    "    else:\n",
    "        test_examples = dataset[\"test\"][:num_examples]\n",
    "    \n",
    "    print(\"\\n=== Global Data Privacy Analysis ===\\n\")\n",
    "    \n",
    "    # Process each example and its corresponding result\n",
    "    for idx, (example, result) in enumerate(zip(test_examples, results)):\n",
    "        # Extract sensitive data from original and ground truth texts\n",
    "        original_text = example['context']\n",
    "        ground_truth = example['anonymized_context']\n",
    "        \n",
    "        # Extract sensitive data using both methods used in detect_privacy_leaks function\n",
    "        def normalize(text):\n",
    "            return re.sub(r'\\s+', ' ', text.lower().strip())\n",
    "            \n",
    "        # Extract all sensitive items from this example\n",
    "        extracted = extract_sensitive_data(original_text, ground_truth)\n",
    "        sensitive_values = extracted['original_values']\n",
    "        \n",
    "        # Method 2: Contextual search (from detect_privacy_leaks)\n",
    "        orig_norm = normalize(original_text)\n",
    "        gt_norm = normalize(ground_truth)\n",
    "        additional_values = []\n",
    "        for match in re.finditer(r'\\[([A-Z_]+)_\\d+\\]', gt_norm):\n",
    "            context = gt_norm[max(0, match.start()-30):min(len(gt_norm), match.end()+30)]\n",
    "            pattern = re.escape(context).replace(re.escape(match.group(0)), '(.*?)')\n",
    "            if found := re.search(pattern, orig_norm):\n",
    "                additional_values.append(found.group(1).strip())\n",
    "        \n",
    "        # Combine values from both methods and filter\n",
    "        all_sensitive_values = sensitive_values.union(set(additional_values))\n",
    "        all_sensitive_values = {v for v in all_sensitive_values if v and len(v) > 3}\n",
    "        \n",
    "        # Use leaked items from the result\n",
    "        leaked_items = result['leaked_items'] if result['sensitive_data_leak'] else []\n",
    "        \n",
    "        # Update counts\n",
    "        example_sensitive_count = len(all_sensitive_values)\n",
    "        example_leak_count = len(leaked_items)\n",
    "        \n",
    "        total_sensitive_items += example_sensitive_count\n",
    "        total_leaked_items += example_leak_count\n",
    "        \n",
    "        # Print detailed information for each example\n",
    "        print(f\"Example {idx+1}:\")\n",
    "        print(f\"  - Sensitive items: {example_sensitive_count}\")\n",
    "        print(f\"  - Leaked items: {example_leak_count}\")\n",
    "        if example_sensitive_count > 0:\n",
    "            print(f\"  - Example privacy score: {(1 - example_leak_count/example_sensitive_count):.2%}\")\n",
    "        else:\n",
    "            print(f\"  - Example privacy score: 100.00%\")\n",
    "        if example_leak_count > 0:\n",
    "            print(f\"  - Leaked values: {', '.join(leaked_items)}\")\n",
    "        print()\n",
    "    \n",
    "    # Calculate global rate\n",
    "    global_leak_rate = total_leaked_items / total_sensitive_items if total_sensitive_items > 0 else 0\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n=== Summary ===\")\n",
    "    print(f\"Total examples analyzed: {len(results)}\")\n",
    "    print(f\"Examples with privacy breaches: {sum(1 for r in results if r['sensitive_data_leak'])}\")\n",
    "    print(f\"Total sensitive items across all examples: {total_sensitive_items}\")\n",
    "    print(f\"Total leaked items across all examples: {total_leaked_items}\")\n",
    "    print(f\"Global privacy protection rate: {(1-global_leak_rate):.2%} of sensitive data properly anonymized\")\n",
    "    \n",
    "    return {\n",
    "        \"total_examples\": len(results),\n",
    "        \"examples_with_leaks\": sum(1 for r in results if r['sensitive_data_leak']),\n",
    "        \"example_leak_rate\": sum(1 for r in results if r['sensitive_data_leak']) / len(results),\n",
    "        \"total_sensitive_items\": total_sensitive_items,\n",
    "        \"total_leaked_items\": total_leaked_items,\n",
    "        \"global_leak_rate\": global_leak_rate\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation on test examples\n",
    "results = evaluate_model_performance(dataset, tokenizer, model, num_examples=500)\n",
    "\n",
    "# Calculate and display average metrics\n",
    "avg_metrics = calculate_average_metrics(results)\n",
    "\n",
    "# Calculate global privacy metrics\n",
    "global_privacy_metrics = calculate_data_privacy_metrics(results, dataset, num_examples=500)\n",
    "\n",
    "print(\"\\n=== Overall Model Performance ===\")\n",
    "print(\"Entity recognition metrics - Precision: {:.3f}, Recall: {:.3f}, F1: {:.3f}\".format(\n",
    "    avg_metrics[\"avg_entity_precision\"], \n",
    "    avg_metrics[\"avg_entity_recall\"], \n",
    "    avg_metrics[\"avg_entity_f1\"])\n",
    ")\n",
    "print(\"BLEU Score: {:.3f}\".format(avg_metrics[\"avg_bleu\"]))\n",
    "print(\"ROUGE-L F1: {:.3f}\".format(avg_metrics[\"avg_rouge_l_f1\"]))\n",
    "print(\"Privacy breaches detected in {:.1%} of examples\".format(avg_metrics[\"leak_percentage\"]))\n",
    "print(\"Global privacy protection rate: {:.2%}\".format(1 - global_privacy_metrics[\"global_leak_rate\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_evaluation_results_demo(results, avg_metrics, global_privacy_metrics):\n",
    "    \"\"\"\n",
    "    Creates simplified visualizations for large evaluation result sets.\n",
    "    \n",
    "    Focused on key metrics and aggregated statistics for demonstration purposes.\n",
    "    \n",
    "    Args:\n",
    "        results: List of evaluation results from evaluate_model_performance\n",
    "        avg_metrics: Dictionary with average metrics\n",
    "        global_privacy_metrics: Dictionary with global privacy metrics\n",
    "    \"\"\"\n",
    "    # Prepare data for analysis\n",
    "    metrics_data = []\n",
    "    for i, r in enumerate(results):\n",
    "        metrics_data.append({\n",
    "            'Entity_F1': r[\"entity_metrics\"][\"f1\"],\n",
    "            'BLEU': r[\"bleu\"],\n",
    "            'ROUGE_L_F1': r[\"rouge_l\"][\"rouge_l_f1\"],\n",
    "            'Has_Leak': 1 if r[\"sensitive_data_leak\"] else 0\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(metrics_data)\n",
    "    \n",
    "    # Set up the figure layout\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Histograms of main metrics\n",
    "    metrics = ['Entity_F1', 'BLEU', 'ROUGE_L_F1']\n",
    "    colors = ['#3498db', '#2ecc71', '#9b59b6']\n",
    "    \n",
    "    for i, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "        axs[0, 0].hist(df[metric], bins=20, alpha=0.7, label=metric.replace('_', ' '), color=color)\n",
    "    \n",
    "    axs[0, 0].axvline(x=avg_metrics['avg_entity_f1'], color='#3498db', linestyle='--')\n",
    "    axs[0, 0].axvline(x=avg_metrics['avg_bleu'], color='#2ecc71', linestyle='--')\n",
    "    axs[0, 0].axvline(x=avg_metrics['avg_rouge_l_f1'], color='#9b59b6', linestyle='--')\n",
    "    \n",
    "    axs[0, 0].set_title('Distribution of Key Metrics', fontsize=14)\n",
    "    axs[0, 0].set_xlabel('Score')\n",
    "    axs[0, 0].set_ylabel('Number of Examples')\n",
    "    axs[0, 0].legend()\n",
    "    axs[0, 0].set_xlim(0, 1)\n",
    "    \n",
    "    # 2. Privacy protection pie chart\n",
    "    sizes = [(global_privacy_metrics['total_sensitive_items'] - global_privacy_metrics['total_leaked_items']), \n",
    "             global_privacy_metrics['total_leaked_items']]\n",
    "    labels = ['Protected', 'Leaked']\n",
    "    colors = ['#3498db', '#e74c3c']\n",
    "    \n",
    "    axs[0, 1].pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', \n",
    "                 startangle=90, shadow=True)\n",
    "    \n",
    "    # Add text in center\n",
    "    privacy_rate = 1 - global_privacy_metrics[\"global_leak_rate\"]\n",
    "    axs[0, 1].text(0, 0, f\"{privacy_rate:.1%}\\nProtected\", \n",
    "                  ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    axs[0, 1].set_title('Global Privacy Protection', fontsize=14)\n",
    "    axs[0, 1].axis('equal')  # Equal aspect ratio\n",
    "    \n",
    "    # 3. Examples with/without leaks\n",
    "    leak_counts = [len(df) - df['Has_Leak'].sum(), df['Has_Leak'].sum()]\n",
    "    labels = ['No Leaks', 'Has Leaks']\n",
    "    colors = ['#2ecc71', '#e74c3c']\n",
    "    \n",
    "    axs[1, 0].bar(labels, leak_counts, color=colors)\n",
    "    axs[1, 0].set_title('Examples with Privacy Breaches', fontsize=14)\n",
    "    axs[1, 0].set_ylabel('Number of Examples')\n",
    "    \n",
    "    # Add percentages on top of bars\n",
    "    for i, count in enumerate(leak_counts):\n",
    "        percentage = 100 * count / len(df)\n",
    "        axs[1, 0].text(i, count + 5, f\"{percentage:.1f}%\", \n",
    "                      ha='center', fontweight='bold')\n",
    "    \n",
    "    # 4. Average metrics comparison\n",
    "    metrics_labels = ['Entity F1', 'BLEU', 'ROUGE-L']\n",
    "    metrics_values = [avg_metrics['avg_entity_f1'], \n",
    "                      avg_metrics['avg_bleu'], \n",
    "                      avg_metrics['avg_rouge_l_f1']]\n",
    "    \n",
    "    axs[1, 1].bar(metrics_labels, metrics_values, color='#3498db')\n",
    "    axs[1, 1].set_title('Average Performance Metrics', fontsize=14)\n",
    "    axs[1, 1].set_ylabel('Score')\n",
    "    axs[1, 1].set_ylim(0, 1)\n",
    "    \n",
    "    # Add scores on top of bars\n",
    "    for i, value in enumerate(metrics_values):\n",
    "        axs[1, 1].text(i, value + 0.03, f\"{value:.3f}\", \n",
    "                      ha='center', fontweight='bold')\n",
    "    \n",
    "    # Adjust layout and add title\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    fig.suptitle('Privacy-Preserving Text Anonymization: Performance Summary', fontsize=16)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print simple summary\n",
    "    print(\"\\n=== Model Performance Summary ===\")\n",
    "    print(f\"Total examples analyzed: {len(results)}\")\n",
    "    print(f\"Entity recognition F1 Score: {avg_metrics['avg_entity_f1']:.3f}\")\n",
    "    print(f\"BLEU Score: {avg_metrics['avg_bleu']:.3f}\")\n",
    "    print(f\"ROUGE-L F1 Score: {avg_metrics['avg_rouge_l_f1']:.3f}\")\n",
    "    print(f\"Privacy breaches in {df['Has_Leak'].mean():.1%} of examples\")\n",
    "    print(f\"Privacy protection rate: {privacy_rate:.1%} of sensitive data anonymized\")\n",
    "\n",
    "# Usage example:\n",
    "visualize_evaluation_results_demo(results, avg_metrics, global_privacy_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical example of using the model on new text\n",
    "def demonstrate_anonymization(text, entity_types):\n",
    "    \"\"\"\n",
    "    Demonstrates the anonymization model on a new text with specified entity types.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to anonymize\n",
    "        entity_types: List of entity types to anonymize (e.g., [\"NAME\", \"DATE\", \"ADDRESS\"])\n",
    "    \"\"\"\n",
    "    print(\"=== Anonymization Demo ===\")\n",
    "    print(\"\\nOriginal text:\")\n",
    "    print(text)\n",
    "    \n",
    "    # Create labels with proper format\n",
    "    formatted_labels = [f\"[{entity.upper()}]\" for entity in entity_types]\n",
    "    \n",
    "    # Generate anonymized version\n",
    "    anonymized = generate_anonymization(text, formatted_labels)\n",
    "    \n",
    "    print(\"\\nAnonymized text:\")\n",
    "    print(anonymized)\n",
    "    \n",
    "    # Detect if any entity types were missed\n",
    "    normalized_types = [t.upper() for t in entity_types]\n",
    "    used_types = []\n",
    "    \n",
    "    for match in re.finditer(r'\\[([A-Z_]+)_\\d+\\]', anonymized):\n",
    "        entity_type = match.group(1)\n",
    "        if entity_type not in used_types:\n",
    "            used_types.append(entity_type)\n",
    "    \n",
    "    missing_types = [t for t in normalized_types if t not in used_types]\n",
    "    \n",
    "    if missing_types:\n",
    "        print(\"\\nWarning: The following entity types were not detected:\")\n",
    "        for t in missing_types:\n",
    "            print(f\"- {t}\")\n",
    "    \n",
    "    print(\"\\nNote: For production use, verify all sensitive information has been properly anonymized.\")\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"On December 15, 2023, Jane Smith (born 05/12/1990) made a payment of $1,245.00 to ABC Corporation from her account #12345678. Please contact her at jane.smith@example.com or call 555-123-4567 if there are any issues.\"\n",
    "\n",
    "demonstrate_anonymization(sample_text, [\"NAME\", \"DATE\", \"EMAIL\", \"PHONE\", \"ACCOUNT_NUMBER\", \"MONEY_AMOUNT\", \"COMPANY\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
