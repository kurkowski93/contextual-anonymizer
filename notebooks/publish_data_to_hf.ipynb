{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publishing Data to Hugging Face Hub\n",
    "\n",
    "This notebook contains code for:\n",
    "- Loading generated synthetic data\n",
    "- Splitting data into train, validation and test sets\n",
    "- Publishing the dataset and its card to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "from huggingface_hub import HfApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads data from generated synthetic data, splits it into train, validation and test sets and pushes it to the hub\n",
    "# Data split proportions:\n",
    "# - 80% training data\n",
    "# - 10% validation data\n",
    "# - 10% test data\n",
    "\n",
    "raw_dataset = load_dataset(\"json\", data_files=\"../data/synthetic_data.json\", field=\"data\", split=\"train\")\n",
    "\n",
    "train_test_dataset = raw_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "train_val_dataset = train_test_dataset[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "final_dataset = DatasetDict({\n",
    "    \"train\": train_test_dataset[\"train\"],\n",
    "    \"validation\": train_val_dataset[\"train\"],\n",
    "    \"test\": train_val_dataset[\"test\"]\n",
    "})\n",
    "\n",
    "print(f\"Training set size: {len(final_dataset['train'])}\")\n",
    "print(f\"Validation set size: {len(final_dataset['validation'])}\")\n",
    "print(f\"Test set size: {len(final_dataset['test'])}\")\n",
    "\n",
    "\n",
    "# commented out because it's already pushed to the hub\n",
    "# final_dataset.push_to_hub(\"kurkowski/synthetic-contextual-anonymizer-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# publish dataset card to the hub\n",
    "api = HfApi()\n",
    "\n",
    "try:\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=\"./../dataset_card.md\",\n",
    "        path_in_repo=\"README.md\",  \n",
    "        repo_id=\"kurkowski/synthetic-contextual-anonymizer-dataset\",\n",
    "        repo_type=\"dataset\"\n",
    "    )\n",
    "    print(\"Dataset card published successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while publishing dataset card: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if the dataset was published correctly\n",
    "verification_dataset = load_dataset(\"kurkowski/synthetic-contextual-anonymizer-dataset\")\n",
    "print(\"Published dataset structure:\")\n",
    "print(verification_dataset)\n",
    "print(\"\\nSample data from each split:\")\n",
    "print(\"\\nTraining set:\")\n",
    "print(final_dataset['train'][0])\n",
    "print(\"\\nValidation set:\")\n",
    "print(final_dataset['validation'][0])\n",
    "print(\"\\nTest set:\")\n",
    "print(final_dataset['test'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
