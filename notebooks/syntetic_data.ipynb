{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Data Generator for Text Anonymization Model Training\n",
    "\n",
    "This notebook is used to generate synthetic training data for text anonymization model. The generated data contains various types of documents (medical, banking, business etc.) along with their anonymized counterparts.\n",
    "\n",
    "### Requirements\n",
    "- Python 3.10+\n",
    "- Installed packages from `requirements.txt`\n",
    "- `.env` file with OpenAI API key (OPENAI_API_KEY)\n",
    "\n",
    "### Data Structure\n",
    "Generated data is saved in JSON format and contains:\n",
    "- Original text\n",
    "- Anonymized version\n",
    "- Used anonymization labels\n",
    "- Metadata (document type, generation date etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Union, Literal\n",
    "from enum import Enum\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "from openai import AsyncOpenAI\n",
    "import asyncio\n",
    "\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core functionalities for synthetic data generation\n",
    "\n",
    "class DocumentType(str, Enum):\n",
    "    \"\"\"\n",
    "    Enum defining supported document types for data generation.\n",
    "    Used to categorize and generate appropriate examples.\n",
    "    \"\"\"\n",
    "    MEDICAL = \"medical\"\n",
    "    BANKING = \"banking\" \n",
    "    BUSINESS = \"business\"\n",
    "    RECRUITMENT = \"recruitment\"\n",
    "    SOCIAL_MEDIA = \"social_media\"\n",
    "    LEGAL = \"legal\"\n",
    "    EDUCATIONAL = \"educational\"\n",
    "    INSURANCE = \"insurance\"\n",
    "    CHAT_PERSONAL = \"chat_personal\"\n",
    "    CHAT_BUSINESS = \"chat_business\"\n",
    "    CHAT_SUPPORT = \"chat_support\"\n",
    "    EMAIL_THREAD = \"email_thread\"\n",
    "\n",
    "class FineTunedDataItem(BaseModel):\n",
    "    \"\"\"\n",
    "    Single training data item containing original and anonymized text.\n",
    "    \n",
    "    Attributes:\n",
    "        context: Original text with sensitive data\n",
    "        anonymized_context: Text with sensitive data replaced by tags\n",
    "        used_labels: String containing list of anonymization tags used\n",
    "    \"\"\"\n",
    "    context: str\n",
    "    anonymized_context: str\n",
    "    used_labels: str\n",
    "    \n",
    "class FineTunedData(BaseModel):\n",
    "    \"\"\"Container for multiple training data items.\"\"\"\n",
    "    items: list[FineTunedDataItem]\n",
    "    \n",
    "def get_document_prompt(doc_type: DocumentType) -> str:\n",
    "    \"\"\"\n",
    "    Get document type specific prompt for GPT model.\n",
    "    \n",
    "    Args:\n",
    "        doc_type: Type of document to generate examples for\n",
    "        \n",
    "    Returns:\n",
    "        String containing prompt tailored for given document type\n",
    "    \"\"\"\n",
    "    prompts = {\n",
    "        DocumentType.MEDICAL: \"\"\"\n",
    "            Generate medical records, patient cards, hospital discharge summaries, and medical test results.\n",
    "            Include multiple participants like doctors, nurses, and patients.\n",
    "        \"\"\",\n",
    "        DocumentType.BANKING: \"\"\"\n",
    "            Generate bank statements, transfer confirmations, loan applications, and credit card documents.\n",
    "            Make sure to include various transaction types and multiple account holders.\n",
    "        \"\"\",\n",
    "        DocumentType.BUSINESS: \"\"\"\n",
    "            Generate business correspondence, invoices, contracts, and company internal documents.\n",
    "            Include various business contexts and multiple stakeholders.\n",
    "        \"\"\",\n",
    "        DocumentType.RECRUITMENT: \"\"\"\n",
    "            Generate CVs, job applications, recommendation letters, and employment contracts.\n",
    "            Include different positions and multiple references.\n",
    "        \"\"\",\n",
    "        DocumentType.SOCIAL_MEDIA: \"\"\"\n",
    "            Generate social media posts, user profiles, and public comments.\n",
    "            Include interactions between multiple users and various post types.\n",
    "        \"\"\",\n",
    "        DocumentType.CHAT_PERSONAL: \"\"\"\n",
    "            Generate personal chat conversations between friends or family members.\n",
    "            Include:\n",
    "            - Multiple participants in the conversation\n",
    "            - Mix of formal and informal language\n",
    "            - References to personal information, dates, and locations\n",
    "            - Message timestamps\n",
    "            - Sharing of contact information\n",
    "            Format as a chat with multiple messages, using timestamps and sender names.\n",
    "        \"\"\",\n",
    "        DocumentType.CHAT_BUSINESS: \"\"\"\n",
    "            Generate business chat conversations between colleagues or business partners.\n",
    "            Include:\n",
    "            - Multiple participants discussing business matters\n",
    "            - References to meetings, projects, and deadlines\n",
    "            - Sharing of business contact information\n",
    "            - Professional language and terms\n",
    "            - Discussion of company-specific information\n",
    "            Format as a business chat with timestamps and professional titles.\n",
    "        \"\"\",\n",
    "        DocumentType.CHAT_SUPPORT: \"\"\"\n",
    "            Generate customer support chat conversations.\n",
    "            Include:\n",
    "            - Customer service representative and customer interaction\n",
    "            - Account verification process\n",
    "            - Personal information sharing for verification\n",
    "            - Problem description and resolution\n",
    "            - Reference numbers and case IDs\n",
    "            Format as a support chat with timestamps and roles.\n",
    "        \"\"\",\n",
    "        DocumentType.EMAIL_THREAD: \"\"\"\n",
    "            Generate email thread conversations with multiple participants.\n",
    "            Include:\n",
    "            - Email headers with addresses and timestamps\n",
    "            - Multiple replies and forwards\n",
    "            - Signature blocks with contact information\n",
    "            - CC and BCC fields\n",
    "            - References to attachments and previous emails\n",
    "            Format as an email thread with proper headers and quotations.\n",
    "        \"\"\",\n",
    "    }\n",
    "    return prompts.get(doc_type, \"Generate examples with personal and sensitive information.\")\n",
    "\n",
    "async def generate_training_data_async(\n",
    "    document_type: DocumentType,\n",
    "    num_examples: int,\n",
    "    additional_labels: List[str] = None,\n",
    "    temperature: float = 0.3\n",
    ") -> FineTunedData:\n",
    "    \"\"\"\n",
    "    Generate synthetic training data for text anonymization model using GPT API.\n",
    "    \n",
    "    This function generates realistic examples of documents containing sensitive data,\n",
    "    along with their anonymized versions where sensitive data is replaced with tags.\n",
    "    \n",
    "    Args:\n",
    "        document_type: Type of documents to generate (medical, banking etc.)\n",
    "        num_examples: Number of examples to generate\n",
    "        additional_labels: Optional list of additional anonymization tags to use\n",
    "        temperature: GPT API temperature parameter for controlling randomness\n",
    "        \n",
    "    Returns:\n",
    "        FineTunedData object containing generated examples\n",
    "        \n",
    "    Example:\n",
    "        >>> data = await generate_training_data_async(\n",
    "        ...     DocumentType.BANKING,\n",
    "        ...     num_examples=5,\n",
    "        ...     additional_labels=[\"CURRENCY\", \"TRANSACTION_ID\"]\n",
    "        ... )\n",
    "    \"\"\"\n",
    "    \n",
    "    client = AsyncOpenAI()\n",
    "    \n",
    "    base_system_message = \"\"\"\n",
    "    You are an assistant that creates examples for training a text anonymization model.\n",
    "    Your task is to generate input-output pairs where:\n",
    "    1. Input text (context) should contain real personal and sensitive data according to GDPR (like real names, emails, addresses etc.)\n",
    "    2. Output text (anonymized_context) should be the same text but with sensitive data replaced with appropriate tags\n",
    "    3. CRITICAL: Tag numbering MUST start from 1 in EACH new example - NEVER continue numbering from previous examples\n",
    "    4. For chat and email threads, maintain the conversation flow and format while anonymizing personal data\n",
    "\n",
    "    IMPORTANT RULES FOR TAG NUMBERING:\n",
    "    - Each example is completely independent (each item in items list is independent)!\n",
    "    - Tag numbering MUST start from 1 in each new example\n",
    "    - NEVER continue numbering from previous examples\n",
    "    - Each type of tag ([NAME], [EMAIL], etc.) starts counting from 1 in each new example\n",
    "\n",
    "    Examples showing correct tag numbering:\n",
    "\n",
    "    Example 1:\n",
    "    Context: \"Hello, my name is John Smith and my email is john.smith@gmail.com. I work with Mary Johnson (mary.j@company.com).\"\n",
    "    Anonymized context: \"Hello, my name is [NAME_1] and my email is [EMAIL_1]. I work with [NAME_2] ([EMAIL_2]).\"\n",
    "\n",
    "    Example 2 (notice numbering starts from 1 again):\n",
    "    Context: \"Hi, I'm Alice Brown and my colleague Bob White can be reached at bob.white@work.com\"\n",
    "    Anonymized context: \"Hi, I'm [NAME_1] and my colleague [NAME_2] can be reached at [EMAIL_1]\"\n",
    "\n",
    "    Example 3 (again, numbering starts from 1):\n",
    "    Context: \"Contact our support: Tom Wilson (tom@support.com) or Jane Davis (jane@support.com)\"\n",
    "    Anonymized context: \"Contact our support: [NAME_1] ([EMAIL_1]) or [NAME_2] ([EMAIL_2])\"\n",
    "\n",
    "    Supported GDPR tags for anonymization (use only in anonymized_context):\n",
    "    - [NAME] - first and last names (use [NAME_1], [NAME_2] etc. within single example)\n",
    "    - [EMAIL] - email addresses (use [EMAIL_1], [EMAIL_2] etc. within single example) \n",
    "    - [PHONE] - phone numbers (use [PHONE_1], [PHONE_2] etc. within single example)\n",
    "    - [ADDRESS] - physical addresses (use [ADDRESS_1], [ADDRESS_2] etc. within single example)\n",
    "    - [PESEL] - Polish national ID numbers\n",
    "    - [NIP] - Polish tax identification numbers\n",
    "    - [DATE] - birth dates and other personal dates (use [DATE_1], [DATE_2] etc. within single example)\n",
    "    - [ACCOUNT] - bank account numbers (use [ACCOUNT_1], [ACCOUNT_2] etc. within single example)\n",
    "    - [USERNAME] - usernames in chats and social media\n",
    "    - [CURRENCY] - monetary amounts with currency \n",
    "    \"\"\"\n",
    "    \n",
    "    if additional_labels:\n",
    "        base_system_message += \"\\nAdditional supported tags for anonymization:\\n\" + \"\\n\".join([f\"- [{label}]\" for label in additional_labels])\n",
    "    \n",
    "    base_system_message += \"\"\"\n",
    "    Generated examples should be realistic and contain real sensitive data according to GDPR in the context field.\n",
    "    The anonymized_context field should contain the same text but with all sensitive data replaced with appropriate tags.\n",
    "    Make sure to use as many different types of sensitive data as possible to have equal distribution of tags in the dataset.\n",
    "    You're not allowed to generate new tag types - use only the provided list.\n",
    "    Remember: Each example is independent - you must start numbering tags from 1 in each new example.\n",
    "    \"\"\"\n",
    "    \n",
    "    document_specific_prompt = get_document_prompt(document_type)\n",
    "    \n",
    "    user_message = f\"Generate {num_examples} realistic examples from {document_type.value} documents. {document_specific_prompt}\"\n",
    "    \n",
    "    completion = await client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": base_system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "        ],\n",
    "        response_format=FineTunedData,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    return completion.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to append new data to the synthetic data file\n",
    "\n",
    "def append_to_synthetic_data(new_data: Union[FineTunedData, List[FineTunedDataItem]], document_type: str = None):\n",
    "    \"\"\"\n",
    "    Appends new data to synthetic_data.json file or creates a new file if it doesn't exist.\n",
    "    \n",
    "    Args:\n",
    "        new_data: New data to add (FineTunedData or list of FineTunedDataItem)\n",
    "        document_type: Document type (optional, for metadata)\n",
    "    \"\"\"\n",
    "    file_path = Path(\"../data/synthetic_data.json\")\n",
    "    \n",
    "    # Convert new_data to list of items if we received FineTunedData\n",
    "    if isinstance(new_data, FineTunedData):\n",
    "        items_to_add = new_data.items\n",
    "    else:\n",
    "        items_to_add = new_data\n",
    "    \n",
    "    # Prepare data structure\n",
    "    if file_path.exists():\n",
    "        # Load existing file\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error in file {file_path}. Creating new file.\")\n",
    "            existing_data = {\n",
    "                \"metadata\": {\n",
    "                    \"creation_date\": datetime.now().isoformat(),\n",
    "                    \"last_update\": datetime.now().isoformat(),\n",
    "                    \"total_examples\": 0,\n",
    "                    \"data_types\": {}\n",
    "                },\n",
    "                \"data\": []\n",
    "            }\n",
    "    else:\n",
    "        # Create new structure\n",
    "        file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        existing_data = {\n",
    "            \"metadata\": {\n",
    "                \"creation_date\": datetime.now().isoformat(),\n",
    "                \"last_update\": datetime.now().isoformat(),\n",
    "                \"total_examples\": 0,\n",
    "                \"data_types\": {}\n",
    "            },\n",
    "            \"data\": []\n",
    "        }\n",
    "    \n",
    "    # Update metadata\n",
    "    existing_data[\"metadata\"][\"last_update\"] = datetime.now().isoformat()\n",
    "    existing_data[\"metadata\"][\"total_examples\"] += len(items_to_add)\n",
    "    \n",
    "    if document_type:\n",
    "        existing_data[\"metadata\"][\"data_types\"][document_type] = \\\n",
    "            existing_data[\"metadata\"][\"data_types\"].get(document_type, 0) + len(items_to_add)\n",
    "    \n",
    "    # Add new data\n",
    "    for item in items_to_add:\n",
    "        if isinstance(item, FineTunedDataItem):\n",
    "            existing_data[\"data\"].append(item.model_dump())\n",
    "        else:\n",
    "            existing_data[\"data\"].append(item)\n",
    "    \n",
    "    # Save updated data\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(existing_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"Added {len(items_to_add)} new examples to {file_path}\")\n",
    "    print(f\"Total number of examples: {existing_data['metadata']['total_examples']}\")\n",
    "    if document_type:\n",
    "        print(f\"Data type distribution: {json.dumps(existing_data['metadata']['data_types'], indent=2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for synthetic data generation\n",
    "# This dictionary defines parameters for different document types:\n",
    "# - First value in tuple: Number of examples to generate\n",
    "# - Second value in tuple: List of additional labels to include in generation\n",
    "doc_config = {\n",
    "    DocumentType.MEDICAL: (550, [\"DIAGNOSIS\", \"MEDICATION\", \"DOCTOR\", \"HOSPITAL\"]),\n",
    "    DocumentType.BANKING: (400, [\"CURRENCY\", \"TRANSACTION_ID\", \"ACCOUNT_TYPE\"]),\n",
    "    DocumentType.BUSINESS: (400, [\"COMPANY\", \"POSITION\", \"DEPARTMENT\"]),\n",
    "    DocumentType.RECRUITMENT: (300, [\"SKILL\", \"EXPERIENCE\", \"POSITION\"]),\n",
    "    DocumentType.SOCIAL_MEDIA: (300, [\"USERNAME\", \"MENTION\"]),\n",
    "    DocumentType.LEGAL: (300, [\"CASE_NUMBER\", \"LAW\", \"COURT\"]),\n",
    "    DocumentType.EDUCATIONAL: (300, [\"SCHOOL\", \"GRADE\", \"DEGREE\"]),\n",
    "    DocumentType.INSURANCE: (300, [\"POLICY_NUMBER\", \"CLAIM_ID\", \"INSURER\"]),\n",
    "    DocumentType.CHAT_PERSONAL: (550, [\"USERNAME\", \"CHAT_ID\"]),\n",
    "    DocumentType.CHAT_BUSINESS: (550, [\"USERNAME\", \"COMPANY\", \"POSITION\", \"PROJECT\"]),\n",
    "    DocumentType.CHAT_SUPPORT: (500, [\"USERNAME\", \"TICKET_ID\", \"ORDER_NUMBER\"]),\n",
    "    DocumentType.EMAIL_THREAD: (400, [\"SUBJECT\", \"THREAD_ID\", \"SIGNATURE\"])\n",
    "}\n",
    "\n",
    "async def generate_batch_async(doc_type: DocumentType, batch_size: int, additional_labels: List[str]):\n",
    "    \"\"\"\n",
    "    Asynchronously generates a batch of synthetic data for a given document type.\n",
    "    \n",
    "    Args:\n",
    "        doc_type (DocumentType): Type of document to generate\n",
    "        batch_size (int): Number of examples to generate in this batch\n",
    "        additional_labels (List[str]): Additional labels to include in generation\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if generation was successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dataset = await generate_training_data_async(\n",
    "            document_type=doc_type,\n",
    "            num_examples=batch_size,\n",
    "            additional_labels=additional_labels,\n",
    "            temperature=0.3  # Lower temperature for more focused generation\n",
    "        )\n",
    "        append_to_synthetic_data(dataset, document_type=doc_type.value)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating data for {doc_type.value}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "async def generate_all_data_async(doc_config: dict):\n",
    "    \"\"\"\n",
    "    Main function to generate all synthetic data asynchronously.\n",
    "    Processes data in batches of 5 examples for better performance and error handling.\n",
    "    \n",
    "    Args:\n",
    "        doc_config (dict): Configuration dictionary defining parameters for each document type\n",
    "        \n",
    "    Prints summary of successful and failed batch generations.\n",
    "    \"\"\"\n",
    "    tasks = []\n",
    "    for doc_type, (num_examples, additional_labels) in doc_config.items():\n",
    "        print(f\"Planning to generate {num_examples} examples for {doc_type.value}...\")\n",
    "        for batch_start in range(0, num_examples, 5):\n",
    "            batch_size = min(5, num_examples - batch_start)\n",
    "            task = asyncio.create_task(\n",
    "                generate_batch_async(doc_type, batch_size, additional_labels)\n",
    "            )\n",
    "            tasks.append(task)\n",
    "    \n",
    "    results = await asyncio.gather(*tasks)\n",
    "    successful = sum(1 for r in results if r)\n",
    "    failed = len(results) - successful\n",
    "    print(f\"\\nData generation completed:\")\n",
    "    print(f\"Successful batches: {successful}\")\n",
    "    print(f\"Failed batches: {failed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await generate_all_data_async(doc_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
